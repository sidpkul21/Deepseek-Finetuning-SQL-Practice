{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29de831d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sidpk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sidpk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Core Libraries ---\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# --- Hugging Face: Dataset, Tokenizer, Model ---\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# --- LoRA & Parameter-Efficient Tuning ---\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "\n",
    "# --- W&B Experiment Tracking ---\n",
    "import wandb\n",
    "\n",
    "# --- SQL Evaluation ---\n",
    "import sqlite3\n",
    "import sqlparse\n",
    "from tabulate import tabulate\n",
    "import evaluate  # for BLEU, ROUGE\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f6810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"text2sql_finetune_and_eval.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f182440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "Using GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"GPU not detected â€” will fall back to CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9fc0eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119876</th>\n",
       "      <td>What was the result of the game that was atten...</td>\n",
       "      <td>CREATE TABLE table_name_47 (\\n    result VARCH...</td>\n",
       "      <td>SELECT result FROM table_name_47 WHERE attenda...</td>\n",
       "      <td>sql_create_context</td>\n",
       "      <td>Below are sql tables schemas paired with instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246771</th>\n",
       "      <td>What is the low money with 374,164$ of debt an...</td>\n",
       "      <td>CREATE TABLE table_54884 (\\n    \"Candidate\" te...</td>\n",
       "      <td>SELECT MIN(\"Money Spent\") FROM table_54884 WHE...</td>\n",
       "      <td>wikisql</td>\n",
       "      <td>Below are sql tables schemas paired with instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82016</th>\n",
       "      <td>What was the Result on 11/02/1946?</td>\n",
       "      <td>CREATE TABLE table_name_84 (\\n    result VARCH...</td>\n",
       "      <td>SELECT result FROM table_name_84 WHERE date = ...</td>\n",
       "      <td>sql_create_context</td>\n",
       "      <td>Below are sql tables schemas paired with instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134146</th>\n",
       "      <td>What is the report for round 9?</td>\n",
       "      <td>CREATE TABLE table_name_9 (\\n    report VARCHA...</td>\n",
       "      <td>SELECT report FROM table_name_9 WHERE round = 9</td>\n",
       "      <td>sql_create_context</td>\n",
       "      <td>Below are sql tables schemas paired with instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28258</th>\n",
       "      <td>For those employees who do not work in departm...</td>\n",
       "      <td>CREATE TABLE job_history (\\n    EMPLOYEE_ID de...</td>\n",
       "      <td>SELECT HIRE_DATE, SUM(SALARY) FROM employees W...</td>\n",
       "      <td>nvbench</td>\n",
       "      <td>Below are sql tables schemas paired with instr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              instruction  \\\n",
       "119876  What was the result of the game that was atten...   \n",
       "246771  What is the low money with 374,164$ of debt an...   \n",
       "82016                  What was the Result on 11/02/1946?   \n",
       "134146                    What is the report for round 9?   \n",
       "28258   For those employees who do not work in departm...   \n",
       "\n",
       "                                                    input  \\\n",
       "119876  CREATE TABLE table_name_47 (\\n    result VARCH...   \n",
       "246771  CREATE TABLE table_54884 (\\n    \"Candidate\" te...   \n",
       "82016   CREATE TABLE table_name_84 (\\n    result VARCH...   \n",
       "134146  CREATE TABLE table_name_9 (\\n    report VARCHA...   \n",
       "28258   CREATE TABLE job_history (\\n    EMPLOYEE_ID de...   \n",
       "\n",
       "                                                 response              source  \\\n",
       "119876  SELECT result FROM table_name_47 WHERE attenda...  sql_create_context   \n",
       "246771  SELECT MIN(\"Money Spent\") FROM table_54884 WHE...             wikisql   \n",
       "82016   SELECT result FROM table_name_84 WHERE date = ...  sql_create_context   \n",
       "134146    SELECT report FROM table_name_9 WHERE round = 9  sql_create_context   \n",
       "28258   SELECT HIRE_DATE, SUM(SALARY) FROM employees W...             nvbench   \n",
       "\n",
       "                                                     text  \n",
       "119876  Below are sql tables schemas paired with instr...  \n",
       "246771  Below are sql tables schemas paired with instr...  \n",
       "82016   Below are sql tables schemas paired with instr...  \n",
       "134146  Below are sql tables schemas paired with instr...  \n",
       "28258   Below are sql tables schemas paired with instr...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"Clinton/Text-to-SQL-v1\")\n",
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "df = pd.DataFrame(shuffled_dataset[\"train\"])\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c0edcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any nulls? instruction    0\n",
      "input          0\n",
      "response       0\n",
      "source         0\n",
      "text           0\n",
      "dtype: int64\n",
      "Any empty strings? instruction    2\n",
      "input          0\n",
      "response       0\n",
      "source         0\n",
      "text           0\n",
      "dtype: int64\n",
      "Unique columns: Index(['instruction', 'input', 'response', 'source', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Any nulls?\", df.isna().sum())\n",
    "print(\"Any empty strings?\", (df == \"\").sum())\n",
    "print(\"Unique columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c82272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 262206\n"
     ]
    }
   ],
   "source": [
    "df_clean = df[df[\"instruction\"] != \"\"].reset_index(drop=True)\n",
    "print(f\"Filtered dataset size: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "919d9e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 235985\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 26221\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "formatted_dataset = Dataset.from_pandas(df_clean[[\"text\"]])\n",
    "formatted_dataset = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(formatted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "245b4c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e64955d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Token Length Stats: {'max': 3226, '95th_percentile': 1432, 'mean': np.float64(376.65), 'min': 66, 'num_samples': 235985}\n",
      "Test Token Length Stats: {'max': 3220, '95th_percentile': 1447, 'mean': np.float64(380.84), 'min': 68, 'num_samples': 26221}\n"
     ]
    }
   ],
   "source": [
    "# Find max length of instructions to pick the optimal max prompt length\n",
    "\n",
    "# Function to compute token length stats\n",
    "def compute_token_stats(dataset_split, tokenizer):\n",
    "    lengths = [len(tokenizer(x)[\"input_ids\"]) for x in dataset_split[\"text\"]]\n",
    "    stats = {\n",
    "        \"max\": int(np.max(lengths)),\n",
    "        \"95th_percentile\": int(np.percentile(lengths, 95)),\n",
    "        \"mean\": round(np.mean(lengths), 2),\n",
    "        \"min\": int(np.min(lengths)),\n",
    "        \"num_samples\": len(lengths),\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "# Compute for both splits\n",
    "train_stats = compute_token_stats(formatted_dataset[\"train\"], tokenizer)\n",
    "test_stats = compute_token_stats(formatted_dataset[\"test\"], tokenizer)\n",
    "\n",
    "print(\"Train Token Length Stats:\", train_stats)\n",
    "print(\"Test Token Length Stats:\", test_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c89640e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean SQL Response token length: 51.61714834900803\n",
      "95th percentile: 162.0\n",
      "Max SQL Response token length: 1868\n"
     ]
    }
   ],
   "source": [
    "#looking at the max token size in the entire data response\n",
    "sql_token_lengths = df_clean[\"response\"].apply(lambda x: len(tokenizer(x, truncation=False)[\"input_ids\"]))\n",
    "\n",
    "# Analyze\n",
    "print(\"Mean SQL Response token length:\", sql_token_lengths.mean())\n",
    "print(\"95th percentile:\", sql_token_lengths.quantile(0.95))\n",
    "print(\"Max SQL Response token length:\", sql_token_lengths.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce08fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smart Padding\n",
    "def tokenize(examples):\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    max_length = 4096\n",
    "\n",
    "    for full_text in examples[\"text\"]:\n",
    "        # Extract prompt and response\n",
    "        prompt_text = full_text.split(\"### Response:\")[0].strip() + \"\\n### Response:\\n\"\n",
    "        response_text = full_text.split(\"### Response:\")[1].strip()\n",
    "        \n",
    "        # Tokenize with truncation\n",
    "        prompt_tokens = tokenizer(prompt_text, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "        response_tokens = tokenizer(response_text, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "        response_tokens.append(tokenizer.eos_token_id)\n",
    "        \n",
    "        # Combine tokens for input\n",
    "        input_ids = prompt_tokens + response_tokens\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        \n",
    "        # Create labels - keep prompt tokens, mask response tokens\n",
    "        labels = input_ids.copy()  # Start with full sequence\n",
    "        labels = [-100] * len(prompt_tokens) + response_tokens #mask prompt tokens\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_mask_list,\n",
    "        \"labels\": labels_list\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # because this is causal LM\n",
    "    pad_to_multiple_of=16  # speeds up training on GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06fa9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "\n",
    "def fix_missing_semicolons(sql_code):\n",
    "    \"\"\"\n",
    "    Inserts semicolons between multiple CREATE TABLE statements if missing.\n",
    "    Looks for patterns like `) CREATE TABLE` and adds a semicolon between them.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\)\\s*(?=CREATE TABLE)', r');\\n', sql_code.strip())\n",
    "\n",
    "def can_execute_sql(generated_sql, schema=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Check if a SQL query or script can be executed against a given schema.\n",
    "\n",
    "    Args:\n",
    "        generated_sql (str): The SQL query or script to test.\n",
    "        schema (str, optional): The database schema to create before testing.\n",
    "        verbose (bool, optional): Whether to print detailed errors.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (bool, str) - (success status, message or error)\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(\":memory:\")\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create schema if provided\n",
    "        if schema:\n",
    "            try:\n",
    "                schema = fix_missing_semicolons(schema)\n",
    "                cursor.executescript(schema)\n",
    "                conn.commit()\n",
    "            except sqlite3.Error as e:\n",
    "                if verbose:\n",
    "                    print(\"Schema execution failed.\")\n",
    "                    print(\"Error:\", e)\n",
    "                return False\n",
    "\n",
    "        # Execute the query or script\n",
    "        try:\n",
    "            if ';' in generated_sql.strip().rstrip(';'):\n",
    "                cursor.executescript(generated_sql)\n",
    "                return True\n",
    "            else:\n",
    "                cursor.execute(generated_sql)\n",
    "                return True\n",
    "        except sqlite3.Error as e:\n",
    "            if verbose:\n",
    "                print(\"Query execution failed.\")\n",
    "                print(\"Error:\", e)\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(\"General error.\")\n",
    "            print(\"Error:\", e)\n",
    "        return False\n",
    "\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c850b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sidpk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\sidpk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\sidpk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#computing the metrics for the baseline model based on similarilty of output, sql compilation and time\n",
    "\n",
    "# Load metrics\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "\n",
    "def extract_sql_from_output(output_text, prompt_text):\n",
    "    \"\"\"Extract SQL query from model output, handling various formats.\"\"\"\n",
    "    # Remove the prompt from the output\n",
    "    sql_text = output_text[len(prompt_text):].strip()\n",
    "    \n",
    "    # Remove any markdown code blocks if present\n",
    "    sql_text = re.sub(r'```sql\\s*|\\s*```', '', sql_text)\n",
    "    sql_text = re.sub(r'```\\s*|\\s*```', '', sql_text)\n",
    "    \n",
    "    # Remove any trailing text after semicolon\n",
    "    if ';' in sql_text:\n",
    "        sql_text = sql_text.split(';')[0] + ';'\n",
    "    \n",
    "    return sql_text.strip()\n",
    "\n",
    "def evaluate_model_on_dataset(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    max_new_tokens=2048\n",
    "):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    compile_success = 0\n",
    "    execution_times = []\n",
    "\n",
    "    dataset_slice = dataset\n",
    "\n",
    "    for example in tqdm(dataset_slice, desc=\"Evaluating\"):\n",
    "        # Extract prompt and response using the same format as tokenize function\n",
    "        prompt_text = example[\"text\"].split(\"### Response:\")[0].strip() + \"\\n### Response:\\n\"\n",
    "        ground_truth = example[\"text\"].split(\"### Response:\")[1].strip()\n",
    "        schema = example[\"text\"].split(\"### Input:\")[1].split(\"### Response:\")[0].strip()\n",
    "\n",
    "        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                max_new_tokens=2048,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "        \n",
    "        # Get the generated SQL - everything after the prompt\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_sql = extract_sql_from_output(decoded, prompt_text)\n",
    "        print(\"SQL Output:\", generated_sql)\n",
    "\n",
    "        # Add prediction for METEOR\n",
    "        predictions.append(generated_sql)\n",
    "        references.append([ground_truth])  # METEOR expects references as a list of lists\n",
    "\n",
    "        # Compile SQL Query and measure time\n",
    "        start_time = time.perf_counter()\n",
    "        success = can_execute_sql(generated_sql, schema)\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        if success:\n",
    "            compile_success += 1\n",
    "            execution_times.append(end_time - start_time)\n",
    "\n",
    "    # Compute metrics\n",
    "    meteor_score = meteor_metric.compute(predictions=predictions, references=references)[\"meteor\"]\n",
    "    sql_compilation_rate = compile_success / len(dataset_slice)\n",
    "    \n",
    "    # Calculate average execution time for successful queries\n",
    "    avg_execution_time = sum(execution_times) / len(execution_times) if execution_times else 0\n",
    "\n",
    "    metrics = {\n",
    "        \"meteor_score\": round(meteor_score, 4),\n",
    "        \"sql_compilation_rate\": round(sql_compilation_rate, 4),\n",
    "        \"avg_execution_time_ms\": round(avg_execution_time * 1000, 2),  # Convert to milliseconds\n",
    "        \"num_eval_samples\": len(dataset_slice),\n",
    "        \"num_successful_queries\": compile_success\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dc566c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "809c73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting the finetuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff42342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "563b4aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msidpkul21\u001b[0m (\u001b[33msidpkul21-georgia-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sidpk\\Documents\\Deepseek-Finetuning-SQL-Practice\\notebooks\\wandb\\run-20250413_010719-yduapox9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune/runs/yduapox9' target=\"_blank\">baseline-run</a></strong> to <a href='https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune' target=\"_blank\">https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune/runs/yduapox9' target=\"_blank\">https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune/runs/yduapox9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune/runs/yduapox9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x14924bdcac0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"deepseek-sql-finetune\",\n",
    "    name=\"baseline-run\",\n",
    "    notes=\"1.3B model with QLoRA, loss tracking\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "371be074",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deepseek-coder-qlora-sql-25k\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=100,\n",
    "    save_steps=1200,\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"deepseek-coder-qlora-sql-25k\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=1200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=10,\n",
    "    max_grad_norm=1.0,\n",
    "    label_names=[\"labels\"] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "617580cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # or [\"query_key_value\"] depending on model architecture\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model_finetune = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce7f2c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a409b49a930c40feaabb7e702f560889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/235985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc577838436843d6a36dd85dbd7a5129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = formatted_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57040a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidpk\\AppData\\Local\\Temp\\ipykernel_17688\\2084438354.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "small_train = tokenized_dataset[\"train\"].select(range(25000))\n",
    "small_eval = tokenized_dataset[\"test\"].select(range(2500))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_finetune,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train, \n",
    "    eval_dataset=small_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e076ce18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 7:47:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.599400</td>\n",
       "      <td>0.681762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.474200</td>\n",
       "      <td>0.612577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.419400</td>\n",
       "      <td>0.584481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.392600</td>\n",
       "      <td>0.571407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.413500</td>\n",
       "      <td>0.563833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.373700</td>\n",
       "      <td>0.560163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>0.558614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9375, training_loss=0.4877398893229167, metrics={'train_runtime': 28033.307, 'train_samples_per_second': 2.675, 'train_steps_per_second': 0.334, 'total_flos': 2.2400043647945933e+17, 'train_loss': 0.4877398893229167, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45a48f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
