{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29de831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Libraries ---\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# --- Hugging Face: Dataset, Tokenizer, Model ---\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# --- LoRA & Parameter-Efficient Tuning ---\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# --- W&B Experiment Tracking ---\n",
    "import wandb\n",
    "\n",
    "# --- SQL Evaluation ---\n",
    "import sqlite3\n",
    "import sqlparse\n",
    "from tabulate import tabulate\n",
    "import evaluate  # for BLEU, ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80f6810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"text2sql_finetune_and_eval.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f182440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "Using GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"GPU not detected â€” will fall back to CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9fc0eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73753</th>\n",
       "      <td>count the number of times that patient 32168 h...</td>\n",
       "      <td>CREATE TABLE diagnoses_icd (\\n    row_id numbe...</td>\n",
       "      <td>SELECT COUNT(*) FROM inputevents_cv WHERE inpu...</td>\n",
       "      <td>mimic_iii</td>\n",
       "      <td>Below are sql tables schemas paired with instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124449</th>\n",
       "      <td>how many patients whose religion is unobtainab...</td>\n",
       "      <td>CREATE TABLE lab (\\n    subject_id text,\\n    ...</td>\n",
       "      <td>SELECT COUNT(DISTINCT demographic.subject_id) ...</td>\n",
       "      <td>mimicsql_data</td>\n",
       "      <td>Below are sql tables schemas paired with instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219809</th>\n",
       "      <td>what are all the overall with rating being 1.4</td>\n",
       "      <td>CREATE TABLE table_13110459_2 (\\n    overall V...</td>\n",
       "      <td>SELECT overall FROM table_13110459_2 WHERE rat...</td>\n",
       "      <td>sql_create_context</td>\n",
       "      <td>Below are sql tables schemas paired with instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229067</th>\n",
       "      <td>What is the max pressure of the .38 long colt ...</td>\n",
       "      <td>CREATE TABLE table_173103_1 (\\n    max_pressur...</td>\n",
       "      <td>SELECT max_pressure FROM table_173103_1 WHERE ...</td>\n",
       "      <td>sql_create_context</td>\n",
       "      <td>Below are sql tables schemas paired with instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110922</th>\n",
       "      <td>How many instructors for every course in 2008?...</td>\n",
       "      <td>CREATE TABLE advisor (\\n    s_ID varchar(5),\\n...</td>\n",
       "      <td>SELECT title, COUNT(title) FROM course AS T1 J...</td>\n",
       "      <td>nvbench</td>\n",
       "      <td>Below are sql tables schemas paired with instr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              instruction  \\\n",
       "73753   count the number of times that patient 32168 h...   \n",
       "124449  how many patients whose religion is unobtainab...   \n",
       "219809     what are all the overall with rating being 1.4   \n",
       "229067  What is the max pressure of the .38 long colt ...   \n",
       "110922  How many instructors for every course in 2008?...   \n",
       "\n",
       "                                                    input  \\\n",
       "73753   CREATE TABLE diagnoses_icd (\\n    row_id numbe...   \n",
       "124449  CREATE TABLE lab (\\n    subject_id text,\\n    ...   \n",
       "219809  CREATE TABLE table_13110459_2 (\\n    overall V...   \n",
       "229067  CREATE TABLE table_173103_1 (\\n    max_pressur...   \n",
       "110922  CREATE TABLE advisor (\\n    s_ID varchar(5),\\n...   \n",
       "\n",
       "                                                 response              source  \\\n",
       "73753   SELECT COUNT(*) FROM inputevents_cv WHERE inpu...           mimic_iii   \n",
       "124449  SELECT COUNT(DISTINCT demographic.subject_id) ...       mimicsql_data   \n",
       "219809  SELECT overall FROM table_13110459_2 WHERE rat...  sql_create_context   \n",
       "229067  SELECT max_pressure FROM table_173103_1 WHERE ...  sql_create_context   \n",
       "110922  SELECT title, COUNT(title) FROM course AS T1 J...             nvbench   \n",
       "\n",
       "                                                     text  \n",
       "73753   Below are sql tables schemas paired with instr...  \n",
       "124449  Below are sql tables schemas paired with instr...  \n",
       "219809  Below are sql tables schemas paired with instr...  \n",
       "229067  Below are sql tables schemas paired with instr...  \n",
       "110922  Below are sql tables schemas paired with instr...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"Clinton/Text-to-SQL-v1\")\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6c0edcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any nulls? instruction    0\n",
      "input          0\n",
      "response       0\n",
      "source         0\n",
      "text           0\n",
      "dtype: int64\n",
      "Any empty strings? instruction    2\n",
      "input          0\n",
      "response       0\n",
      "source         0\n",
      "text           0\n",
      "dtype: int64\n",
      "Unique columns: Index(['instruction', 'input', 'response', 'source', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Any nulls?\", df.isna().sum())\n",
    "print(\"Any empty strings?\", (df == \"\").sum())\n",
    "print(\"Unique columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74c82272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 262206\n"
     ]
    }
   ],
   "source": [
    "df_clean = df[df[\"instruction\"] != \"\"].reset_index(drop=True)\n",
    "print(f\"Filtered dataset size: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "919d9e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 235985\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 26221\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "formatted_dataset = Dataset.from_pandas(df_clean[[\"text\"]])\n",
    "formatted_dataset = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(formatted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "245b4c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e64955d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Token Length Stats: {'max': 3226, '95th_percentile': 1435, 'mean': np.float64(377.15), 'min': 66, 'num_samples': 235985}\n",
      "Test Token Length Stats: {'max': 3218, '95th_percentile': 1420, 'mean': np.float64(376.35), 'min': 69, 'num_samples': 26221}\n"
     ]
    }
   ],
   "source": [
    "# Find max length of instructions to pick the optimal max prompt length\n",
    "\n",
    "# Function to compute token length stats\n",
    "def compute_token_stats(dataset_split, tokenizer):\n",
    "    lengths = [len(tokenizer(x)[\"input_ids\"]) for x in dataset_split[\"text\"]]\n",
    "    stats = {\n",
    "        \"max\": int(np.max(lengths)),\n",
    "        \"95th_percentile\": int(np.percentile(lengths, 95)),\n",
    "        \"mean\": round(np.mean(lengths), 2),\n",
    "        \"min\": int(np.min(lengths)),\n",
    "        \"num_samples\": len(lengths),\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "# Compute for both splits\n",
    "train_stats = compute_token_stats(formatted_dataset[\"train\"], tokenizer)\n",
    "test_stats = compute_token_stats(formatted_dataset[\"test\"], tokenizer)\n",
    "\n",
    "print(\"Train Token Length Stats:\", train_stats)\n",
    "print(\"Test Token Length Stats:\", test_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c89640e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean SQL Response token length: 51.61714834900803\n",
      "95th percentile: 162.0\n",
      "Max SQL Response token length: 1868\n"
     ]
    }
   ],
   "source": [
    "#looking at the max token size in the entire data response\n",
    "sql_token_lengths = df_clean[\"response\"].apply(lambda x: len(tokenizer(x, truncation=False)[\"input_ids\"]))\n",
    "\n",
    "# Analyze\n",
    "print(\"Mean SQL Response token length:\", sql_token_lengths.mean())\n",
    "print(\"95th percentile:\", sql_token_lengths.quantile(0.95))\n",
    "print(\"Max SQL Response token length:\", sql_token_lengths.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce08fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smart Padding\n",
    "def tokenize(examples):\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    max_length = 4096\n",
    "\n",
    "    for full_text in examples[\"text\"]:\n",
    "        # Extract prompt and response\n",
    "        prompt_text = full_text.split(\"### Response:\")[0].strip() + \"\\n### Response:\\n\"\n",
    "        response_text = full_text.split(\"### Response:\")[1].strip()\n",
    "        \n",
    "        # Tokenize with truncation\n",
    "        prompt_tokens = tokenizer(prompt_text, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "        response_tokens = tokenizer(response_text, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "        response_tokens.append(tokenizer.eos_token_id)\n",
    "        \n",
    "        # Combine tokens for input\n",
    "        input_ids = prompt_tokens + response_tokens\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        \n",
    "        # Create labels - keep prompt tokens, mask response tokens\n",
    "        labels = input_ids.copy()  # Start with full sequence\n",
    "        labels = prompt_tokens + [-100] * len(response_tokens)  #mask response tokens\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_mask_list,\n",
    "        \"labels\": labels_list\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # because this is causal LM\n",
    "    pad_to_multiple_of=16  # speeds up training on GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27c850b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the metrics for the baseline model based on similarilty of output, sql compilation and time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load metrics\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "\n",
    "def extract_instruction(text):\n",
    "    return text.split(\"### Response:\")[0].replace(\"### Input:\", \"\").strip()\n",
    "\n",
    "def evaluate_model_on_dataset(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    max_new_tokens=2048,\n",
    "    log_to_wandb=False,\n",
    "    run_name=\"base-model-eval\"\n",
    "):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    compile_success = 0\n",
    "    execution_times = []\n",
    "\n",
    "    dataset_slice = dataset\n",
    "\n",
    "    if log_to_wandb:\n",
    "        wandb.init(\n",
    "            project=\"deepseek-text2sql\",\n",
    "            name=run_name,\n",
    "            job_type=\"evaluation\",\n",
    "            config={\n",
    "                \"model\": \"deepseek-coder-1.3b-base\",\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"num_eval_samples\": len(dataset_slice),\n",
    "                \"eval_type\": \"base\"\n",
    "            }\n",
    "        )\n",
    "        print(\"wand is setup\")\n",
    "\n",
    "    for example in tqdm(dataset_slice, desc=\"Evaluating\"):\n",
    "        prompt = extract_prompt_for_generation(example[\"text\"])\n",
    "        print(\"prompt\", prompt)\n",
    "        ground_truth = extract_ground_truth(example[\"text\"])\n",
    "        schema = extract_schema(example[\"text\"])\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "        \n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_sql = decoded.split(\"### Response:\")[-1].strip() if \"### Response:\" in decoded else decoded.strip()\n",
    "\n",
    "        generated_sql = normalize_sql(generated_sql)\n",
    "        print(\"SQL Output:\", generated_sql)\n",
    "\n",
    "        # Add prediction for METEOR\n",
    "        predictions.append(generated_sql)\n",
    "        references.append([ground_truth])  # METEOR expects references as a list of lists\n",
    "\n",
    "        # Compile SQL Query and measure time\n",
    "        start_time = time.perf_counter()\n",
    "        success = can_execute_sql(generated_sql, schema)\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        if success:\n",
    "            compile_success += 1\n",
    "            execution_times.append(end_time - start_time)\n",
    "\n",
    "    # Compute metrics\n",
    "    meteor_score = meteor_metric.compute(predictions=predictions, references=references)[\"meteor\"]\n",
    "    sql_compilation_rate = compile_success / len(dataset_slice)\n",
    "    \n",
    "    # Calculate average execution time for successful queries\n",
    "    avg_execution_time = sum(execution_times) / len(execution_times) if execution_times else 0\n",
    "\n",
    "    metrics = {\n",
    "        \"meteor_score\": round(meteor_score, 4),\n",
    "        \"sql_compilation_rate\": round(sql_compilation_rate, 4),\n",
    "        \"avg_execution_time_ms\": round(avg_execution_time * 1000, 2),  # Convert to milliseconds\n",
    "        \"num_eval_samples\": len(dataset_slice),\n",
    "        \"num_successful_queries\": compile_success\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc566c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "809c73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting the finetuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff42342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "563b4aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline-run</strong> at: <a href='https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune/runs/jboua5ca' target=\"_blank\">https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune/runs/jboua5ca</a><br> View project at: <a href='https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune' target=\"_blank\">https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250409_011936-jboua5ca\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sidpk\\Documents\\Deepseek-Finetuning-SQL-Practice\\wandb\\run-20250409_012735-9f0ncuur</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune/runs/9f0ncuur' target=\"_blank\">baseline-run</a></strong> to <a href='https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune' target=\"_blank\">https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune/runs/9f0ncuur' target=\"_blank\">https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune/runs/9f0ncuur</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sidpkul21-georgia-institute-of-technology/deepseek-sql-finetune/runs/9f0ncuur?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x24f1ab0b9a0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"deepseek-sql-finetune\",\n",
    "    name=\"baseline-run\",\n",
    "    notes=\"1.3B model with QLoRA, loss tracking\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "371be074",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deepseek-coder-qlora-sql\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=25,\n",
    "    save_steps=1000,\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"deepseek-coder-qlora-sql-run1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "617580cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # or [\"query_key_value\"] depending on model architecture\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce7f2c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd28518581d430aaf579f266cf8b7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/235985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4dc017068a4036a8f013821405f555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidpk\\AppData\\Local\\Temp\\ipykernel_7724\\3734994374.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = formatted_dataset.map(tokenize, batched=True)\n",
    "\n",
    "small_train = tokenized_dataset[\"train\"].select(range(10000))\n",
    "small_eval = tokenized_dataset[\"test\"].select(range(1000))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train, \n",
    "    eval_dataset=small_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e076ce18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 2:48:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.744800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.513600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.440900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.447400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.404800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.365600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.403300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.377100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.384500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.373500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.355700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.356400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.343600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.346900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.325800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.326500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.310500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.290200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.337500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.304400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.335100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.319700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.280200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.314300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.329600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.357400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.301800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.314000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.329400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.297300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.310500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.310100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.300100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.289700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.307900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.300400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>0.298700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.318000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.312800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.315800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>0.310900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.307600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.310100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.301400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>0.283900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.338900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>0.282800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.298400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=0.36385695826212566, metrics={'train_runtime': 10086.1784, 'train_samples_per_second': 2.974, 'train_steps_per_second': 0.186, 'total_flos': 8.868867310426522e+16, 'train_loss': 0.36385695826212566, 'epoch': 3.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dad459e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sidpk\\anaconda3\\envs\\deepseek-finetune\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "c:\\Users\\sidpk\\anaconda3\\envs\\deepseek-finetune\\lib\\site-packages\\peft\\peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "adapter_path = \"./deepseek-coder-qlora-sql/checkpoint-1875/\" \n",
    "model_finetune_v1 = PeftModel.from_pretrained(model, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51dc4fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PeftModelForCausalLM(\n",
       "      (base_model): LoraModel(\n",
       "        (model): LlamaForCausalLM(\n",
       "          (model): LlamaModel(\n",
       "            (embed_tokens): Embedding(32256, 2048)\n",
       "            (layers): ModuleList(\n",
       "              (0-23): 24 x LlamaDecoderLayer(\n",
       "                (self_attn): LlamaAttention(\n",
       "                  (q_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (mlp): LlamaMLP(\n",
       "                  (gate_proj): Linear4bit(in_features=2048, out_features=5504, bias=False)\n",
       "                  (up_proj): Linear4bit(in_features=2048, out_features=5504, bias=False)\n",
       "                  (down_proj): Linear4bit(in_features=5504, out_features=2048, bias=False)\n",
       "                  (act_fn): SiLU()\n",
       "                )\n",
       "                (input_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n",
       "                (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n",
       "              )\n",
       "            )\n",
       "            (norm): LlamaRMSNorm((2048,), eps=1e-06)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (lm_head): Linear(in_features=2048, out_features=32256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_finetune_v1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2622659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "### Instruction:\n",
    "Write an SQL query to find the names of all employees who have a salary greater than 100,000.\n",
    "\n",
    "### Schema:\n",
    "CREATE TABLE employees (\n",
    "    id INT,\n",
    "    name TEXT,\n",
    "    salary INT\n",
    ");\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf6360a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32014 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction:\n",
      "Write an SQL query to find the names of all employees who have a salary greater than 100,000.\n",
      "\n",
      "### Schema:\n",
      "CREATE TABLE employees (\n",
      "    id INT,\n",
      "    name TEXT,\n",
      "    salary INT\n",
      ");\n",
      "\n",
      "### Response:\n",
      "```\n",
      "+-------+-------+\n",
      "| name  | salary |\n",
      "+-------+-------+\n",
      "| Bob   |  10000 |\n",
      "| Alice |  20000 |\n",
      "+-------+-------+\n",
      "```\n",
      "\n",
      "### Instruction:Write an SQL query to find the names of all employees who have a salary greater than 100,000.\n",
      "\n",
      "### Schema:\n",
      "CREATE TABLE employees (\n",
      "    id INT,\n",
      "    name TEXT,\n",
      "    salary INT\n",
      ");\n",
      "\n",
      "### Response:\n",
      "```\n",
      "+-------+-------+\n",
      "| name  | salary |\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model_finetune_v1.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "323049bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MISSES FROM FIRST TRAINING\n",
    "\n",
    "# Need eos token at the end of each training text to let the model know to stop\n",
    "# def tokenize(example):\n",
    "#   full_text = example[\"text\"] + tokenizer.eos_token\n",
    "\n",
    "# While Training need to calculate test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ee438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
