{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29de831d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sidpk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sidpk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Core Libraries ---\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# --- Hugging Face: Dataset, Tokenizer, Model ---\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# --- LoRA & Parameter-Efficient Tuning ---\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# --- W&B Experiment Tracking ---\n",
    "import wandb\n",
    "\n",
    "# --- SQL Evaluation ---\n",
    "import sqlite3\n",
    "import sqlparse\n",
    "from tabulate import tabulate\n",
    "import evaluate  # for BLEU, ROUGE\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f6810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"text2sql_finetune_and_eval.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f182440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "Using GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"GPU not detected â€” will fall back to CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9fc0eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103181</th>\n",
       "      <td>how long does it take to get from DENVER to OA...</td>\n",
       "      <td>CREATE TABLE code_description (\\n    code varc...</td>\n",
       "      <td>SELECT DISTINCT flight.time_elapsed FROM airpo...</td>\n",
       "      <td>atis</td>\n",
       "      <td>Below are sql tables schemas paired with instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201659</th>\n",
       "      <td>What are the dates that have an average sea le...</td>\n",
       "      <td>CREATE TABLE weather (\\n    date text,\\n    ma...</td>\n",
       "      <td>SELECT date FROM weather WHERE mean_sea_level_...</td>\n",
       "      <td>spider</td>\n",
       "      <td>Below are sql tables schemas paired with instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220751</th>\n",
       "      <td>How many articles were published in the Cell j...</td>\n",
       "      <td>CREATE TABLE keyphrase (\\n    keyphraseid int,...</td>\n",
       "      <td>SELECT DISTINCT COUNT(paper.paperid) FROM jour...</td>\n",
       "      <td>scholar</td>\n",
       "      <td>Below are sql tables schemas paired with instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36701</th>\n",
       "      <td>What are the international tourist arrivals in...</td>\n",
       "      <td>CREATE TABLE table_14752049_2 (\\n    internati...</td>\n",
       "      <td>SELECT international_tourist_arrivals__2010_ F...</td>\n",
       "      <td>sql_create_context</td>\n",
       "      <td>Below are sql tables schemas paired with instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179685</th>\n",
       "      <td>how did patient 99135 last be admitted in 2104...</td>\n",
       "      <td>CREATE TABLE diagnoses_icd (\\n    row_id numbe...</td>\n",
       "      <td>SELECT admissions.admission_type FROM admissio...</td>\n",
       "      <td>mimic_iii</td>\n",
       "      <td>Below are sql tables schemas paired with instr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              instruction  \\\n",
       "103181  how long does it take to get from DENVER to OA...   \n",
       "201659  What are the dates that have an average sea le...   \n",
       "220751  How many articles were published in the Cell j...   \n",
       "36701   What are the international tourist arrivals in...   \n",
       "179685  how did patient 99135 last be admitted in 2104...   \n",
       "\n",
       "                                                    input  \\\n",
       "103181  CREATE TABLE code_description (\\n    code varc...   \n",
       "201659  CREATE TABLE weather (\\n    date text,\\n    ma...   \n",
       "220751  CREATE TABLE keyphrase (\\n    keyphraseid int,...   \n",
       "36701   CREATE TABLE table_14752049_2 (\\n    internati...   \n",
       "179685  CREATE TABLE diagnoses_icd (\\n    row_id numbe...   \n",
       "\n",
       "                                                 response              source  \\\n",
       "103181  SELECT DISTINCT flight.time_elapsed FROM airpo...                atis   \n",
       "201659  SELECT date FROM weather WHERE mean_sea_level_...              spider   \n",
       "220751  SELECT DISTINCT COUNT(paper.paperid) FROM jour...             scholar   \n",
       "36701   SELECT international_tourist_arrivals__2010_ F...  sql_create_context   \n",
       "179685  SELECT admissions.admission_type FROM admissio...           mimic_iii   \n",
       "\n",
       "                                                     text  \n",
       "103181  Below are sql tables schemas paired with instr...  \n",
       "201659  Below are sql tables schemas paired with instr...  \n",
       "220751  Below are sql tables schemas paired with instr...  \n",
       "36701   Below are sql tables schemas paired with instr...  \n",
       "179685  Below are sql tables schemas paired with instr...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"Clinton/Text-to-SQL-v1\")\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6c0edcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any nulls? instruction    0\n",
      "input          0\n",
      "response       0\n",
      "source         0\n",
      "text           0\n",
      "dtype: int64\n",
      "Any empty strings? instruction    2\n",
      "input          0\n",
      "response       0\n",
      "source         0\n",
      "text           0\n",
      "dtype: int64\n",
      "Unique columns: Index(['instruction', 'input', 'response', 'source', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Any nulls?\", df.isna().sum())\n",
    "print(\"Any empty strings?\", (df == \"\").sum())\n",
    "print(\"Unique columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74c82272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 262206\n"
     ]
    }
   ],
   "source": [
    "df_clean = df[df[\"instruction\"] != \"\"].reset_index(drop=True)\n",
    "print(f\"Filtered dataset size: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "919d9e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 235985\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 26221\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "formatted_dataset = Dataset.from_pandas(df_clean[[\"text\"]])\n",
    "formatted_dataset = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(formatted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "245b4c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e64955d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Token Length Stats: {'max': 3226, '95th_percentile': 1435, 'mean': np.float64(377.15), 'min': 66, 'num_samples': 235985}\n",
      "Test Token Length Stats: {'max': 3218, '95th_percentile': 1420, 'mean': np.float64(376.35), 'min': 69, 'num_samples': 26221}\n"
     ]
    }
   ],
   "source": [
    "# Find max length of instructions to pick the optimal max prompt length\n",
    "\n",
    "# Function to compute token length stats\n",
    "def compute_token_stats(dataset_split, tokenizer):\n",
    "    lengths = [len(tokenizer(x)[\"input_ids\"]) for x in dataset_split[\"text\"]]\n",
    "    stats = {\n",
    "        \"max\": int(np.max(lengths)),\n",
    "        \"95th_percentile\": int(np.percentile(lengths, 95)),\n",
    "        \"mean\": round(np.mean(lengths), 2),\n",
    "        \"min\": int(np.min(lengths)),\n",
    "        \"num_samples\": len(lengths),\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "# Compute for both splits\n",
    "train_stats = compute_token_stats(formatted_dataset[\"train\"], tokenizer)\n",
    "test_stats = compute_token_stats(formatted_dataset[\"test\"], tokenizer)\n",
    "\n",
    "print(\"Train Token Length Stats:\", train_stats)\n",
    "print(\"Test Token Length Stats:\", test_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c89640e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean SQL Response token length: 51.61714834900803\n",
      "95th percentile: 162.0\n",
      "Max SQL Response token length: 1868\n"
     ]
    }
   ],
   "source": [
    "#looking at the max token size in the entire data response\n",
    "sql_token_lengths = df_clean[\"response\"].apply(lambda x: len(tokenizer(x, truncation=False)[\"input_ids\"]))\n",
    "\n",
    "# Analyze\n",
    "print(\"Mean SQL Response token length:\", sql_token_lengths.mean())\n",
    "print(\"95th percentile:\", sql_token_lengths.quantile(0.95))\n",
    "print(\"Max SQL Response token length:\", sql_token_lengths.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce08fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smart Padding\n",
    "def tokenize(examples):\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    max_length = 4096\n",
    "\n",
    "    for full_text in examples[\"text\"]:\n",
    "        # Extract prompt and response\n",
    "        prompt_text = full_text.split(\"### Response:\")[0].strip() + \"\\n### Response:\\n\"\n",
    "        response_text = full_text.split(\"### Response:\")[1].strip()\n",
    "        \n",
    "        # Tokenize with truncation\n",
    "        prompt_tokens = tokenizer(prompt_text, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "        response_tokens = tokenizer(response_text, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "        response_tokens.append(tokenizer.eos_token_id)\n",
    "        \n",
    "        # Combine tokens for input\n",
    "        input_ids = prompt_tokens + response_tokens\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        \n",
    "        # Create labels - keep prompt tokens, mask response tokens\n",
    "        labels = input_ids.copy()  # Start with full sequence\n",
    "        labels = prompt_tokens + [-100] * len(response_tokens)  #mask response tokens\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_mask_list,\n",
    "        \"labels\": labels_list\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # because this is causal LM\n",
    "    pad_to_multiple_of=16  # speeds up training on GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27c850b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sidpk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\sidpk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\sidpk\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "#computing the metrics for the baseline model based on similarilty of output, sql compilation and time\n",
    "\n",
    "# Load metrics\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "\n",
    "def can_execute_sql(generated_sql, schema=None):\n",
    "    \"\"\"Check if a SQL query can be executed against a given schema.\n",
    "    \n",
    "    Args:\n",
    "        generated_sql (str): The SQL query to test\n",
    "        schema (str, optional): The database schema to create before testing\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the query executes successfully, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(\":memory:\")\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create schema if provided\n",
    "        if schema:\n",
    "            cursor.executescript(schema)\n",
    "            \n",
    "        # Try to execute the query\n",
    "        cursor.execute(generated_sql)\n",
    "        return True\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"SQL Error: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def extract_sql_from_output(output_text, prompt_text):\n",
    "    \"\"\"Extract SQL query from model output, handling various formats.\"\"\"\n",
    "    # Remove the prompt from the output\n",
    "    sql_text = output_text[len(prompt_text):].strip()\n",
    "    \n",
    "    # Remove any markdown code blocks if present\n",
    "    sql_text = re.sub(r'```sql\\s*|\\s*```', '', sql_text)\n",
    "    sql_text = re.sub(r'```\\s*|\\s*```', '', sql_text)\n",
    "    \n",
    "    # Remove any trailing text after semicolon\n",
    "    if ';' in sql_text:\n",
    "        sql_text = sql_text.split(';')[0] + ';'\n",
    "    \n",
    "    return sql_text.strip()\n",
    "\n",
    "def evaluate_model_on_dataset(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    max_new_tokens=2048\n",
    "):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    compile_success = 0\n",
    "    execution_times = []\n",
    "\n",
    "    dataset_slice = dataset\n",
    "\n",
    "    for example in tqdm(dataset_slice, desc=\"Evaluating\"):\n",
    "        # Extract prompt and response using the same format as tokenize function\n",
    "        prompt_text = example[\"text\"].split(\"### Response:\")[0].strip() + \"\\n### Response:\\n\"\n",
    "        ground_truth = example[\"text\"].split(\"### Response:\")[1].strip()\n",
    "        schema = example[\"text\"].split(\"### Input:\")[1].split(\"### Response:\")[0].strip()\n",
    "\n",
    "        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "        \n",
    "        # Get the generated SQL - everything after the prompt\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_sql = extract_sql_from_output(decoded, prompt_text)\n",
    "        print(\"SQL Output:\", generated_sql)\n",
    "\n",
    "        # Add prediction for METEOR\n",
    "        predictions.append(generated_sql)\n",
    "        references.append([ground_truth])  # METEOR expects references as a list of lists\n",
    "\n",
    "        # Compile SQL Query and measure time\n",
    "        start_time = time.perf_counter()\n",
    "        success = can_execute_sql(generated_sql, schema)\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        if success:\n",
    "            compile_success += 1\n",
    "            execution_times.append(end_time - start_time)\n",
    "\n",
    "    # Compute metrics\n",
    "    meteor_score = meteor_metric.compute(predictions=predictions, references=references)[\"meteor\"]\n",
    "    sql_compilation_rate = compile_success / len(dataset_slice)\n",
    "    \n",
    "    # Calculate average execution time for successful queries\n",
    "    avg_execution_time = sum(execution_times) / len(execution_times) if execution_times else 0\n",
    "\n",
    "    metrics = {\n",
    "        \"meteor_score\": round(meteor_score, 4),\n",
    "        \"sql_compilation_rate\": round(sql_compilation_rate, 4),\n",
    "        \"avg_execution_time_ms\": round(avg_execution_time * 1000, 2),  # Convert to milliseconds\n",
    "        \"num_eval_samples\": len(dataset_slice),\n",
    "        \"num_successful_queries\": compile_success\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1dc566c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "809c73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting the finetuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff42342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"deepseek-sql-finetune\",\n",
    "    name=\"baseline-run\",\n",
    "    notes=\"1.3B model with QLoRA, loss tracking\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "371be074",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deepseek-coder-qlora-sql\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=25,\n",
    "    save_steps=1000,\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"deepseek-coder-qlora-sql-run1\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=3,\n",
    "    label_names=[\"labels\"] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "617580cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # or [\"query_key_value\"] depending on model architecture\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model_finetune = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce7f2c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0289d940f2aa4ede9dd70201ca7fa2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/235985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bdaaa3a15a5408db075f4ef2e572116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = formatted_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57040a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidpk\\AppData\\Local\\Temp\\ipykernel_20540\\3699285566.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "small_train = tokenized_dataset[\"train\"].select(range(1000))\n",
    "small_eval = tokenized_dataset[\"test\"].select(range(100))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_finetune,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train, \n",
    "    eval_dataset=small_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e076ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad459e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "adapter_path = \"./deepseek-coder-qlora-sql/???\"\n",
    "model_finetune_v1 = PeftModel.from_pretrained(model, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_finetune_v1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2622659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "### Instruction:\n",
    "Write an SQL query to find the names of all employees who have a salary greater than 100,000.\n",
    "\n",
    "### Schema:\n",
    "CREATE TABLE employees (\n",
    "    id INT,\n",
    "    name TEXT,\n",
    "    salary INT\n",
    ");\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6360a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model_finetune_v1.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "323049bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MISSES FROM FIRST TRAINING\n",
    "\n",
    "# Need eos token at the end of each training text to let the model know to stop\n",
    "# def tokenize(example):\n",
    "#   full_text = example[\"text\"] + tokenizer.eos_token\n",
    "\n",
    "# While Training need to calculate test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ee438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
